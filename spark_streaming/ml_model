import pandas as pd
# --- Báº®T Äáº¦U PHáº¦N SPARK ---
# 1. Khá»Ÿi táº¡o SparkSession
# PHáº¢I KHá»I Táº O TRÆ¯á»šC KHI Sá»¬ Dá»¤NG BIáº¾N 'spark'
from pyspark.sql import SparkSession
from pyspark.sql.functions import regexp_replace, col, expr
from pyspark.sql.types import IntegerType
from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF
from pyspark.ml import Pipeline, PipelineModel
from pyspark.ml.classification import MultilayerPerceptronClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator # Äá»ƒ Ä‘Ã¡nh giÃ¡
import os
import shutil

# Khá»Ÿi táº¡o Spark Session
spark = SparkSession.builder.appName("TextClassification").getOrCreate()
print("\nSparkSession Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o.")

# --- Äá»‹nh nghÄ©a hÃ m Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh ---
# Äá»ŠNH NGHÄ¨A HÃ€M TRÆ¯á»šC KHI Gá»ŒI NÃ“
def evaluate_model(predictions, model_name):
    print(f"\nğŸ“Š ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh: {model_name}")
    # Evaluator sá»­ dá»¥ng label_ml vÃ  prediction
    acc_eval = MulticlassClassificationEvaluator(labelCol="label_ml", predictionCol="prediction", metricName="accuracy")
    f1_eval = MulticlassClassificationEvaluator(labelCol="label_ml", predictionCol="prediction", metricName="f1")
    prec_eval = MulticlassClassificationEvaluator(labelCol="label_ml", predictionCol="prediction", metricName="weightedPrecision")
    rec_eval = MulticlassClassificationEvaluator(labelCol="label_ml", predictionCol="prediction", metricName="weightedRecall")

    print(f"âœ… Accuracy:  {acc_eval.evaluate(predictions):.4f}")
    print(f"ğŸ¯ F1-score:  {f1_eval.evaluate(predictions):.4f}")
    print(f"ğŸ” Precision: {prec_eval.evaluate(predictions):.4f}")
    print(f"ğŸ” Recall:    {rec_eval.evaluate(predictions):.4f}")


# 2. Chuyá»ƒn tá»« Pandas DataFrame (df) sang Spark DataFrame
# Äáº£m báº£o biáº¿n 'df' (Pandas DataFrame) Ä‘Ã£ Ä‘Æ°á»£c táº¡o tá»« cÃ¡c bÆ°á»›c Ä‘á»c dá»¯ liá»‡u á»Ÿ trÃªn
try:
    df_spark = spark.createDataFrame(df)
    print("ÄÃ£ chuyá»ƒn Pandas DataFrame sang Spark DataFrame.")
except NameError:
    print("Lá»—i: Pandas DataFrame 'df' chÆ°a Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a. Vui lÃ²ng cháº¡y cÃ¡c Ã´ Ä‘á»c dá»¯ liá»‡u trÆ°á»›c.")
    # Dá»«ng thá»±c thi náº¿u df chÆ°a tá»“n táº¡i
    # raise NameError("Pandas DataFrame 'df' is not defined")
except Exception as e:
    print(f"Lá»—i khi chuyá»ƒn Pandas DataFrame sang Spark DataFrame: {e}")


# --- CÃ¡c bÆ°á»›c chuáº©n bá»‹ dá»¯ liá»‡u ban Ä‘áº§u trÃªn Spark DataFrame ---
# Äáº£m báº£o df_spark cÃ³ cá»™t 'content' vÃ  'label'
df_spark = df_spark.withColumn("content_clean", regexp_replace("content", "[\\r\\n]+", " "))

# Ã‰p kiá»ƒu label tá»« CSV thÃ nh sá»‘ nguyÃªn (náº¿u cáº§n, Ä‘áº£m báº£o cá»™t 'label' tá»“n táº¡i vÃ  cÃ³ giÃ¡ trá»‹ sá»‘)
# Dá»±a trÃªn code cá»§a báº¡n, cá»™t 'label' Ä‘Ã£ cÃ³ tá»« dá»¯ liá»‡u ES hoáº·c temp.csv
# Äáº£m báº£o ráº±ng cá»™t 'label' nÃ y chá»©a cÃ¡c giÃ¡ trá»‹ sá»‘ nguyÃªn (-1, 0, 1)
# Náº¿u cá»™t label tá»« CSV lÃ  string, báº¡n cáº§n Ã¡nh xáº¡ nÃ³ sang sá»‘ trÆ°á»›c khi cast
df_spark = df_spark.withColumn("rating", col("label").cast(IntegerType()))

# Táº¡o label dÆ°Æ¡ng: label = rating + 1 (Ä‘á»ƒ phÃ¹ há»£p yÃªu cáº§u mÃ´ hÃ¬nh Spark ML - nhÃ£n pháº£i lÃ  sá»‘ nguyÃªn khÃ´ng Ã¢m 0, 1, 2...)
df_spark = df_spark.withColumn("label_ml", expr("rating + 1")) # Sá»­ dá»¥ng label_ml cho mÃ´ hÃ¬nh Spark ML

# Loáº¡i bá» cÃ¡c dÃ²ng cÃ³ rating null sau khi cast
df_spark = df_spark.filter(col("rating").isNotNull())

print("ÄÃ£ chuáº©n bá»‹ dá»¯ liá»‡u trÃªn Spark DataFrame.")
df_spark.printSchema()
print(f"Sá»‘ dÃ²ng sau khi chuáº©n bá»‹: {df_spark.count()}")


# --- CÃ¡c bÆ°á»›c tiá»n xá»­ lÃ½ vÄƒn báº£n ---
# INPUT: content_clean
# OUTPUT: features
tokenizer = Tokenizer(inputCol="content_clean", outputCol="words")
remover = StopWordsRemover(inputCol="words", outputCol="filtered")
# Sá»­ dá»¥ng numFeatures=1000 Ä‘á»ƒ khá»›p vá»›i input_dim cá»§a MLP
hashingTF = HashingTF(inputCol="filtered", outputCol="rawFeatures", numFeatures=1000)
idf = IDF(inputCol="rawFeatures", outputCol="features")

# --- MÃ´ hÃ¬nh MLP ---
# INPUT: features
# OUTPUT: prediction, probability, rawPrediction
# Sá»‘ chiá»u Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o pháº£i khá»›p vá»›i numFeatures cá»§a HashingTF
input_dim = 1000 # <-- KHá»šP Vá»šI HashingTF numFeatures
# Cáº¥u trÃºc máº¡ng: [input, hidden1, hidden2, ..., output]
# Sá»‘ lá»›p output pháº£i khá»›p vá»›i sá»‘ lÆ°á»£ng nhÃ£n (3 lá»›p: 0, 1, 2)
layers = [input_dim, 64, 32, 3]

mlp = MultilayerPerceptronClassifier(
    featuresCol="features", # Äáº§u vÃ o lÃ  cá»™t 'features' tá»« IDF
    labelCol="label_ml", # NhÃ£n lÃ  cá»™t 'label_ml' (0, 1, 2) cho mÃ´ hÃ¬nh Spark ML
    maxIter=100,
    layers=layers,
    blockSize=128,
    seed=42
)

# --- XÃ¢y dá»±ng Pipeline HOÃ€N CHá»ˆNH ---
# Káº¿t há»£p Táº¤T Cáº¢ cÃ¡c bÆ°á»›c tiá»n xá»­ lÃ½ vÃ  mÃ´ hÃ¬nh ML thÃ nh má»™t Pipeline DUY NHáº¤T
# Thá»© tá»± cÃ¡c stages ráº¥t quan trá»ng: Tokenizer -> Remover -> HashingTF -> IDF -> MLP
full_pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, mlp])

print("\nÄÃ£ xÃ¢y dá»±ng Pipeline ML hoÃ n chá»‰nh (Tiá»n xá»­ lÃ½ + MÃ´ hÃ¬nh).")

# --- Chia train/test ---
# Chia dá»¯ liá»‡u sau khi Ä‘Ã£ lÃ m sáº¡ch ban Ä‘áº§u vÃ  táº¡o label_ml
train_data, test_data = df_spark.randomSplit([0.8, 0.2], seed=42)

print(f"Dá»¯ liá»‡u Ä‘Ã£ chia: Train={train_data.count()} dÃ²ng, Test={test_data.count()} dÃ²ng")

# --- Huáº¥n luyá»‡n Pipeline HOÃ€N CHá»ˆNH trÃªn dá»¯ liá»‡u huáº¥n luyá»‡n ---
# Pipeline sáº½ tá»± Ä‘á»™ng cháº¡y cÃ¡c bÆ°á»›c tiá»n xá»­ lÃ½ vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh MLP
print("\nBáº¯t Ä‘áº§u huáº¥n luyá»‡n Pipeline ML hoÃ n chá»‰nh...")
# Lá»—i Py4J thÆ°á»ng xáº£y ra á»Ÿ bÆ°á»›c fit() nÃ y do tÃ i nguyÃªn hoáº·c mÃ´i trÆ°á»ng Colab
full_pipeline_model = full_pipeline.fit(train_data)

print("Pipeline ML hoÃ n chá»‰nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n.")

# --- (TÃ¹y chá»n) ÄÃ¡nh giÃ¡ Pipeline Ä‘Ã£ huáº¥n luyá»‡n trÃªn test_data ---
# Ãp dá»¥ng toÃ n bá»™ Pipeline lÃªn test_data Ä‘á»ƒ dá»± Ä‘oÃ¡n
test_predictions = full_pipeline_model.transform(test_data)

# Chuyá»ƒn prediction vá» láº¡i rating gá»‘c (-1, 0, 1)
# Cá»™t 'prediction' Ä‘Æ°á»£c táº¡o ra bá»Ÿi mÃ´ hÃ¬nh phÃ¢n loáº¡i trong Pipeline
test_predictions = test_predictions.withColumn("ml_predicted_rating", col("prediction") - 1)

print("\nKáº¿t quáº£ dá»± Ä‘oÃ¡n trÃªn Test Data báº±ng Pipeline Ä‘Ã£ huáº¥n luyá»‡n:")
# Hiá»ƒn thá»‹ cÃ¡c cá»™t quan trá»ng: ná»™i dung gá»‘c, rating gá»‘c, rating dá»± Ä‘oÃ¡n, label dá»± Ä‘oÃ¡n ML, xÃ¡c suáº¥t
test_predictions.select("content", "rating", "ml_predicted_rating", "prediction", "probability").show(truncate=60)

# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh (sá»­ dá»¥ng label_ml vÃ  prediction)
# HÃ m evaluate_model Ä‘Ã£ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a á»Ÿ trÃªn
evaluate_model(test_predictions, "Full ML Pipeline (DNN)")


# --- LÆ°u Pipeline Model Ä‘Ã£ huáº¥n luyá»‡n ---
# LÆ°u toÃ n bá»™ Pipeline Model ra má»™t thÆ° má»¥c
# Thay tháº¿ Ä‘Æ°á»ng dáº«n dÆ°á»›i Ä‘Ã¢y báº±ng Ä‘Æ°á»ng dáº«n báº¡n muá»‘n lÆ°u trong mÃ´i trÆ°á»ng Colab
SAVE_PATH_COLAB = "/content/spark_full_ml_pipeline_model" # <-- ÄÆ¯á»œNG DáºªN LÆ¯U TRONG COLAB

try:
    # XÃ³a thÆ° má»¥c cÅ© náº¿u tá»“n táº¡i
    if os.path.exists(SAVE_PATH_COLAB):
        shutil.rmtree(SAVE_PATH_COLAB)
        print(f"ÄÃ£ xÃ³a thÆ° má»¥c mÃ´ hÃ¬nh cÅ© táº¡i {SAVE_PATH_COLAB}")

    full_pipeline_model.save(SAVE_PATH_COLAB)
    print(f"Pipeline ML hoÃ n chá»‰nh Ä‘Ã£ Ä‘Æ°á»£c lÆ°u thÃ nh cÃ´ng táº¡i: {SAVE_PATH_COLAB}")

except Exception as e:
    print(f"Lá»—i khi lÆ°u Pipeline ML: {e}")
    print(f"Vui lÃ²ng kiá»ƒm tra quyá»n ghi vÃ o thÆ° má»¥c {SAVE_PATH_COLAB} trong Colab.")
